# Web Scraping LinkedIn Data

![GitHub repo size](https://img.shields.io/github/repo-size/pragana12/web_scraping_linkedin?style=for-the-badge)
![GitHub language count](https://img.shields.io/github/languages/count/pragana12/web_scraping_linkedin?style=for-the-badge)
![GitHub forks](https://img.shields.io/github/forks/pragana12/web_scraping_linkedin?style=for-the-badge)
![Bitbucket open issues](https://img.shields.io/bitbucket/issues/pragana12/web_scraping_linkedin?style=for-the-badge)
![Bitbucket open pull requests](https://img.shields.io/bitbucket/pr-raw/pragana12/web_scraping_linkedin?style=for-the-badge)

> Um script de Web Scraping desenvolvido em Python usando Selenium para extrair dados de vagas no **LinkedIn**..

Este projeto tem como objetivo automatizar a extra√ß√£o de dados de vagas de emprego no LinkedIn. O script realiza o login, navega at√© a p√°gina de vagas, e coleta informa√ß√µes relevantes sobre cada vaga, como t√≠tulo, empresa, localiza√ß√£o, descri√ß√£o, compet√™ncias necess√°rias, e o ramo da empresa..

## Ajustes e melhorias

O projeto ainda est√° em desenvolvimento e as pr√≥ximas atualiza√ß√µes ser√£o voltadas nas seguintes tarefas:

- [x] Extra√ß√£o e inser√ß√£o de dados no banco de dados MySQL.
- [x] Manipula√ß√£o e limpeza de dados extra√≠dos.
- [ ] Adi√ß√£o de suporte para diferentes tipos de vagas e crit√©rios de pesquisa.
- [ ] Melhorias na detec√ß√£o de elementos na p√°gina.

## üíª Pr√©-requisitos

Antes de come√ßar, verifique se voc√™ atendeu aos seguintes requisitos:

1. Certifique-se de ter Python instalado no seu ambiente.
2. Instale as bibliotecas necess√°rias executando pip install -r requirements.txt.
3. Certifique-se de ter Um banco de dados MySQL configurado conforme as informa√ß√µes fornecidas no script.
4. Execute os scripts utilizando um ambiente Python."

## üöÄ Instalando depend√™ncias neces√°rias do projeto

Para instalar as depend√™ncias neces√°rias do projeto, siga estas etapas:

Linux e macOS:

```
<pip install -r requirements.txt.>
```

Windows:

```
<pip install -r requirements.txt.>
```

## üìÇ Conte√∫do do Reposit√≥rio

1. [Script - Web Scraping](scripts/web_scraping_linkedin.py)
   - Realiza a extra√ß√£o de dados de vagas no LinkedIn.
   - Insere os dados no banco de dados MySQL.
   - Exibe informa√ß√µes detalhadas sobre cada vaga.

## ‚òï Usando o Projeto

Para usar o projeto, siga estas etapas:

```### Configura√ß√£o do Ambiente:

Certifique-se de ter Python instalado no seu ambiente.
Instale as bibliotecas necess√°rias executando `pip install -r requirements.txt`.
Certifique-se de ter o arquivo telecom_churn.csv na pasta raiz do script.

### Execu√ß√£o dos Scripts:

1. Execute os scripts utilizando um ambiente Python.
2. O script realizar√° automaticamente o login, acessar√° a p√°gina de vagas e extrair√° as informa√ß√µes.

 # Observa√ß√£o:
- O script foi projetado para a extra√ß√£o de dados espec√≠ficos. Modifica√ß√µes podem ser necess√°rias para diferentes necessidades de extra√ß√£o.

```

## üì´ Contribuindo para data_science_circo

Para contribuir com este projeto, siga estas etapas:

1. **Bifurque este reposit√≥rio.**
2. **Crie um branch:** `git checkout -b <nome_branch>`.
3. **Fa√ßa suas altera√ß√µes e confirme-as:** `git commit -m '<mensagem_commit>'`
4. **Envie para o branch original:** `git push origin <nome_do_projeto>/<nome_branch>`
5. **Crie a solicita√ß√£o de pull.**

Lembre-se de seguir as boas pr√°ticas de contribui√ß√£o e fornecer uma descri√ß√£o clara das altera√ß√µes que voc√™ est√° propondo. Se voc√™ estiver resolvendo um problema existente, mencione o n√∫mero da issue relacionada.

Como alternativa, consulte a documenta√ß√£o do GitHub em [como criar uma solicita√ß√£o pull](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request).

### Informa√ß√µes Adicionais

- Se voc√™ tiver d√∫vidas ou sugest√µes, abra uma [issue](https://github.com/pragana12/web_scraping_linkedin/issues).
- Antes de criar uma nova solicita√ß√£o de pull, certifique-se de sincronizar seu branch com o branch principal do reposit√≥rio original.

### ü§ù Agradece√ßo suas contribui√ß√µes
